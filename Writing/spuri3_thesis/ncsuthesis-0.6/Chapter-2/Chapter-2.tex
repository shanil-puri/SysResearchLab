\chapter{Motivation and Background}
\label{chap-two}
\section{Motivation}
Computation reuse across executions benefits programs with long computation time most, especially converging algorithms that require multiple iterations to compute desired results. This class of problems as such has no generic polynomial time algorithms, thus making history reuse for predictive initialization a good candidate for optimization. Another benefit may be improved accuracy. Through appropriate history result reuse, numbers of iterations of computations could be saved while the accuracy gets improved. The key point of computation reuse on different data is finding suitable history information to reuse. Thus, the above stated problem essentially boils down to introducing a probabilistic method of calculating data set similarities quickly and accurately. With databases containing a number of instances, data set features is a key component of each instance, and history computation result of the program on this data set is the value. Then, through computing distance from current data set to each instance, our framework could select the instance, which has the highest probability to provide most effective computation reuse.
Optimization of converging Algorithms such as K-means clustering and SGD based SVM are highly beneficial. Algorithms of this category are used frequently and have a multitude of applications in real world machine learning and big data processing.  Some of the motivations for this work may be listed as follows:
\begin{itemize}
	\item \textit{Frequently Used:} These algorithms are used frequently to tackle real world problems and thus even small improvements can have a significant impact.
	\item \textit{Wide Area of Application} These algorithms also have wide areas of real world application ranging from machine learning to big data problems.
	\item \textit{Prime Candidates:} These algorithms are ideal suited for such optimizations, as their speed of convergence and accuracy is directly dependent on the starting points for the algorithm and thus can be used as proof for the benefits of history reuse easily.
\end{itemize}
\section{Background and Related Work}
For most part of the generation and the testing of our \textit{History Reuse architecture} we use the classical \textit{K-Means algorithm (Lloyd's Algorithm)}\cite{kmeans_orig} . We then test the architecture with the \textit{Stochastic Gradient Decent based Simple Vector Machines}\cite{sgd_svm} to prove the global viability of our algorithm.
The classic K-Means algorithm (Lloyd’s algorithm) consists of two steps. For an input of 'n' data points of 'd' dimensions and 'k' initial cluster centers, the assignment step assigns each point to its closest cluster, and the update step updates each of the k cluster centers with the centroid of the points assigned to that cluster. The algorithm repeats until all the cluster centers remain unchanged in a single iteration.
Because of its simplicity and general applicability, the algorithm is one of the most widely used clustering algorithms in practice, and is identified as one of the top 10 data mining algorithms (Wu et al., 2008). However, when n , k, or d is large, the algorithm runs slow due to its linear dependence on n, k, and d. There have been a number of efforts trying to improve its speed. Some try to come up with better initial centers (e.g. K-Means{++} \cite{kmeans++} or parallel implementations \cite{kmens_parellel}. This thesis will look to present more on this approach by exploring an avenue not much pursued before, namely historical data set cluster center reuse for K-Means Initialization. 
Prior efforts in this direction include: K-Means{++}(Arthur and Vassilvitskii, 2007; Bahmani et al., 2012), K-Means Initialization Methods for Improving Clustering by Simulated Annealing (Gabriela Trazzi Perim et al. 2008), an optimized initialization center K-Means clustering algorithm based on density (Xiaofeng Zhou et al. 2015). These prior methods, while having made a significant contribution, have failed to replace the Lloyd`s algorithm which still remains the dominant choice in practice exemplified by the implementations in popular libraries, such as GraphLab (Low et al.), OpenCV, ml-pack (Curtin et al. 2013) and so on.
Previous implementations that have tried to optimize the selection of the initial centroids based only on the current data set. For instance, the original K-Means Algorithm (Lloyd’s algorithm) chooses \textit{k} cluster centers randomly from the points available in the current data set. The K-Means{++} \cite{kmeans++} further optimizes this by taking steps to increase probability of getting good starting points by first choosing a random centroid and then proceeding to choose the furthest possible centroid from last chosen centroid iteratively, doing this for each centroid computation. The approximation method for initialization \cite{approx_meth}, aims to approximate the selection of centroid, but this approach produces clustering results different from the results of the standard K-Means. The above algorithms as can be seen work only on the current data set at hand. No prior work has directly tried to systematically exploit historical data for computation, which forms the basis of this thesis.
This work will introduce and formalize the History Reuse Architecture. We will then use \textit{History Reuse} to introduce a new Initialization methodology for the K-Means algorithm: "Historical dataset center reuse for K means initialization", an enhanced K means implementation which aims to optimize the K means algorithm by aiming to choose the best possible starting points for the K-Means\cite{kmeans_orig} for faster convergence. Since the only modification that are being made are in the initialization step of the algorithm it stands to reason that the algorithm would continue to uphold the same standards as the standard K-Means algorithm. We will also use our History Reuse Architecture and test it for SGD based SVM\cite{sgd_svm} to prove the global application of our generic architecture.

While history reuse has been prevalent and an area of great exploration as of late, this approach of utilizing offline computed training data for History Reuse is unique and as such has not been explored. History Reuse though has been seen in myriad other works including Yinyang K-Means \cite{yinyang_kmeans} where in pre-computed geometrical information for distances for points from centroids is stored and reused in the multiple iterations of the algorithm in a single run. It makes use of this information for both initialization and re-labeling of points by reusing distance computation information. Similarly, the K-means optimized by Elkan \cite{Elkan03usingthe} and by Drake and Hamerly\cite{drake} also computes the triangular inequality and uses it to optimize run time by minimizing computations of distance per iteration by reusing the precalculated distance bounds with changes in the limit of the distances maintained in history on a per iteration basis.

Compiler studies have also had a lot of work done in the area of history reuse for code optimization such as the "Automated Locality Optimization Based on the Reuse Distance of String Operations" \cite{auto_cache_opt} which aims to use call context on Cache hits for optimal use of L2/L3 caches. These and other works\cite{value_reuse_extended}\cite{load_reuse_proceeding}\cite{load_reuse_proceeding_article}\cite{locality_block_reuse} have often used history reuse in the optimization at the micro or program level. 

Some of the other specific works in optimizing the K-Means algorithm \cite{kmeans_orig}, which is also a product of this thesis, have used myriad approaches for optimizing the K-Means algorithm. Some optimizations use approximation methodologies (\cite{Czumaj}; \cite{Sculley}; \cite{4270197}; \cite{Guha:1998:CEC:276305.276312}; \cite{Zeng:2012:FAK:2354409.2354758}) while other try to speed up K-Means inherently, while trying to maintain the semantics of the original algorithm. An example of the latter is to speed up the algorithm using KD-Trees \cite{Pelleg:1999:AEK:312129.312248}\cite{Kanungo:2002:EKC:628329.628801} which show promise for smaller cluster sizes but do not perform as well for larger cluster sizes.

With our proposed solution we extend this approach to a macro level choosing to look at the problem at the data level as compared to the optimizations done at program level. Since our approach is program independent to a large extent, our algorithm will work as a generic framework for all data related optimizations and can be directly combined with any program level optimization to achieve further speedups. For instance, we may use our approach in conjunction with the above mentioned Yinyang K-Means implementation to optimize the initialization as well iteration time for a single run of the algorithm. This way we stand to gain the best of both worlds by combining both program level and data level optimizations.


\section{Important Terminology}
For the propose of our discussion we assume data to be represented in a \textit{2-D matrix} where in each \textit{row} represents a \textit{single point} in a data set while the \textit{columns} are used to represent the \textit{feature set} of each point.
The following are important terms for this work and are used through out the later chapters:
\begin{itemize}
	\item \textbf{Data Source}: These are the actual sources of Data Set repositories from which we source our data for testing purposes.
	\item \textbf{Data Set}: Data on which the actual algorithm is run after dividing the data from the \textit{Data Source}. Each \textit{Data Set} is built up of multiple \textit{Data Items or Data Points}.
	\begin{itemize}
		\item \textit{Current Data Set:} Represents the data set on which the current iteration of the algorithm is to be run.
		\item \textit{Historic Data Set:} Data Set present in the History Data Base for which results have been computed in previous iterations of the algorithm, making the data set one of the viable candidates for \textit{History Reuse} for \textit{Current Data Set.}
		\item \textit{Data Item / Data Point:} Single row in Data set. Represents a single data point in the larger \textit{Data Set}.
		\item \textit{Dimensionality(d):} Number of columns used to represent a single \textit{Data Item.}
	\end{itemize}
	\item \textbf{History Data Base:} Data base for storing all data sets that may be candidates for \textit{History Reuse} for \textit{current data set} and for which final results have been calculated in previous iterations of the algorithm.
	\item \textbf{Cluster Count (k):} The total number of cluster in which the \textit{Data Set} is to be clustered when using the \textit{K-Means Algorithm}.
	\item \textbf{SGD based SVM}: Stochastic Gradient Decent Based Simple Vector Machine. \cite{sgd_svm}
	\item \textbf{PCA: } Principal Component Analysis. \cite{pca}\cite{pca_visual}	
\end{itemize}


