\chapter{Challenges}
\label{chap-three}
History Reuse seems an intuitive solution to many problems. While the concept in itself is simple enough: reuse some computations for previous runs for current run of an algorithm, the implementation of the same provides quite a challenge. Some of the major challenges faced are: Data Feature Abstraction, Similarity definition, Scalability, and Reuse.
\section{Data Feature Abstraction}
Each \textit{Data Set} is categorized by a set of \textit{features} in the form of columns, assuming the data set is represented as a 2D array. In such a case not all features for the data set hold equal importance in the data set categorization. Thus the first challenge faced by us was ensuring that we use the most important features only for computation of \textit{History Reuse Data Set.} 
Use of too many features for the computation of our Historic Data Sets may affect efficiency, while on the flip side, use of too few data set features may result in the loss of the meaning of the data set itself thus invalidating its candidacy for History Reuse. This thus was one of the major challenges face by us in the computation of Historical Data Sets for Computation reuse.
\section{Similarity Computation}
Another important feature requirement for our problem statement was to come up with a uniform metric for feature set comparison. For this we propose a “Probability based” metric for similarity between data sets. By this metric we can make a quick yet accurate assessment regarding the degree of similarity in between data sets. Obviously the best choice of historical data set would be the one with the highest probability of being similar to the current data set.
Defining such a metric though poses its own challenges, namely: data sets used for comparison may have different scale (may not be normalized), may not have the same important features or may not be along the same axis of projection. These and more issues make the definition of a Similarity metric a difficult task. Failure to solve all the challenges mentioned above would lead to the failure of our \textit{probabilistic similarity metric.}
Following sub sections will shed a little more light on the challenges in :
\subsection{Non Uniform Scale}
Data sets present in the \textit{History Data Base} and the current data set may not have the same scale, i.e. distributions and variances. This essentially means, any similarity computations between the two data set would essentially be meaningless as computation reuse across such data sets may be impossible. This thus presents the first challenge of normalizing the data sets on to the same plane to provide a common platform for similarity computation.
\subsection{Different Feature Sets: Feature Set Abstraction}
Another major challenge faced is that the most influential features present in \textit{historical data sets} and the current data set may be very different. Since the class of algorithms targeted by our algorithm is often highly dependent upon the feature sets for final results, the matching of data sets based on only the most important features becomes imperative for good reuse results. This, thus presents the challenge of analyzing both the current data set and the historical data set for the extraction of the most important features. This must be done in real-time and must be both efficient and effective. as mentioned in \textit{section 1}, we must also ensure the optimal use of the features to ensure meaning full comparisons.
\subsection{Rotated Data}
Often the axis of projection for current and historical data may often in different planes, and while the distributions may be similar for data, their being on different planes altogether makes similarity computation challenging. Thus one of the major challenges in computing similarity was to ensure both \textit{historic and current data set} be in the same plane. This again must be done at run time, so as to ensure that both the historic and current data and the historic data are on the same plane (\textit{plane} for current data is known only at run time.) This further poses the challenge of an efficient method for the planar normalization of the data sets being compared.
\section{Efficient and effective comparison}
The above challenges seen clearly pose the challenge of efficiency and affectivity. The loss of either of the two would essentially entail the failure of any proposed algorithm.
Efficiency is desired since a lot of the above challenges must be solved in real-time as they require analysis of the data set used in the current iteration of the algorithm.
Effective is desired since a non-effective data set may provide a bad historical data set for computation reuse. Our experimentation has shown us, use of badly matched historical data sets tend to cause severe punishment in terms of both run time and quality of results for our tested algorithms.
\section{How to use the historical datasets}
The last challenge faced is how to reuse computation from the selected history data set itself. For example in case of the K-Means algorithm we get both the labels, as well as cluster centroids for historical data sets. The use of labels though quicker in initialization may produce different results depending upon the order of the data points in the data set.  Another example maybe, the data item count for historical data set may be different from the current data set and thus using labels directly may simple cause the failure of the algorithm to divide the data into the required number of clusters. On the flip side too few data items in historic data set may cause some data items in current data set to not being assigned any cluster at all. The reuse of computed centroids to regenerate labels on the other hand, while slower, will always ensure the correct label initialization irrespective of the order of the data points in the data set, or the count of the required data labels etc.
Thus we can see that the correct use of Historical data is an equally challenging problem.

