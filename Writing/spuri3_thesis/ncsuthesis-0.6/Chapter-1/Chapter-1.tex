\chapter{INTRODUCTION}
\label{chap-one}
In this data explosion era, there are myriad programs that are executed repeatedly on large sets of data, consuming high amounts of energy and resources. Critical process reused on different data sets will no doubt help reduce the time and computations. In this work we will introduce a new probabilistic model of measuring data similarities. Based on these data similarities we will show how critical computations may be reused across data sets, saving energy and improving performance.

Our work will introduce a general architecture for comparing two or more data sets and get a probabilistic measure of similarity. We reason that if two data sets are similar, computation reuse from previous iteration of an algorithm for the similar data set will provide a good speed up in the execution of the algorithm for the current data set.
We first define the architecture to compute the above-mentioned metric of similarity and then proceed to validate our above-mentioned stipulation for the K-Means and the SGD based SVM algorithms.

In these explorations, we found challenges in 4 aspects: \textbf{data features, similarity definition, scalability,} and \textbf{reuse}. Data features, is the description of data sets. Similarity definition (distance between two data sets) is the metric we use to describe similarity between data sets. Data feature and distance definition are used together for reuse instance selection from a program specific database. The selection of history record is the most essential part of computation reuse.

Since the method used to calculate distances between two data sets, should also be defined based on data features, the problem becomes even more complex. Scalability issues are related with number of instances in database, size of the target data set, and dimension of data-set. Goal of computation reuse is to save computation time and energy. In order to select a suitable history record, some extra computation is inevitably introduced. The dilemma is the trade-off between the amount of computation reuse and the introduced overhead. 

Reuse, thus is the process to decide what history information the program should reuse and how to reuse the pre-computed information.
In this study we introduce the concept of Historical Computation reuse. This is an Intuitive Idea whose exploration for the family of algorithms mentioned above is spotted at best.

\textbf{\textit{Computation Reuse}} is the process of deciding what history information the program should reuse and how to best reuse the pre-computed information.
While the idea is simple and intuitive in nature, it promises big gains if correctly implemented with a wide variety of uses.
In this study we give an empirical study, showing that effectively computation reuse could enhance program performance.
Although, the idea of computation reuse is simple, there are many difficulties need to be solved to achieve efficient and effective reuse. 
In our explorations, we found out challenges in 4 aspects: data feature abstraction, similarity definition, scalability, and reuse. 
\textit{Data Features}, is the description of data sets. The major challenged we faced in \textit{Data Feature Abstraction} was to reduce dimensionality (for the sake of optimization), while preserving the integrity of data.
\textit{Similarity definition} (distance between two data-sets) is the way we would like to describe similarity between two data set. The challenge was to come up with a meaningful metric to give an accurate prediction of suitability for reuse. 
We used \textit{Data feature abstraction} and \textit{Similarity definition} together for reuse instance selection from a program specific database. The selection of history data set is the most essential part of computation reuse because different data sets will lead to dramatically different computation times. Because the most important properties of the input data may vary on different programs, it is difficult to decide a universal data feature and data distance definition. Since the method used to calculate distances between two data sets descriptions also should be defined based on the data features, the problem becomes even more complicated. 
\textit{Scalability} issues are related with number of data set instances in database, size of the target data set, and its dimensionality. Goal of computation reuse is to save computation time and energy. In order to select a suitable history record, some extra computation is inevitably introduced into the computation. The dilemma is the tradeoff between the amount of computation reuse and the introduced overhead. For example, increase the number of instances in database will increase the probability of finding a good history record. However, it will also increase the introduced selection overhead. The size of target data set and the dimensionality of would also produce similar issues.
\textit{Reuse} is the process of deciding what history information the program should reuse and how to reuse this pre-computed information. Challenges in this field are mainly caused by the differences among programs and algorithms. For a specific program or algorithm, it might be a trivial solution while for another selecting historical data may be a complex problem. Our approach of building a general framework to work as a plug and play device makes the process even more complicated. 
In this work, we investigate multiple solutions to address each of the challenges, and come up a framework, which we will apply to two of commonly used algorithms, namely: \textbf{\textit{K-Means(Lloydâ€™s Clustering algorithm)}} and the \textbf{\textit{Stochastic Gradient Decent based Support Vector Machine}} for our explorations and providing proof of effectiveness of History Reuse and to validate the efficiency and the effectiveness of our algorithm.
The paper is organized in the following sections: \textit{\ref{chap-two}} gives motivation, background and definitions of terms we use in this paper. \textit{\ref{chap-three}} will give formal definition to the challenges we faced and our approach for coming to a successful conclusion. \textit{\ref{chap-four}} will formalize the framework and give the formal algorithm for our architecture. We will also discuss the other approaches we used to come to our conclusions and give reasons for their failures. We present the evaluations for our algorithm in \textit{\ref{chap-five}}. Related and future work is discussed in \textit{\ref{chap-six}}, while \textit{\ref{chap-seven}} provides conclusion derived from our explorations.
